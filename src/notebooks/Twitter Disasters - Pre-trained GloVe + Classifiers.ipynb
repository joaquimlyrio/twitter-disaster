{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Disaster Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import linear_model, model_selection, preprocessing, metrics, svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "\n",
    "# tensorflow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "train = pd.read_csv( '../data/nlp-getting-started/train.csv' )\n",
    "test  = pd.read_csv( '../data/nlp-getting-started/test.csv' )\n",
    "subm_samp = pd.read_csv( '../data/nlp-getting-started/sample_submission.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Class 1 Size : 3271\n",
      " Class 0 Size : 4342\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Checking size of each class on train set\n",
    "print( ' Class 1 Size : ' + str(train[ train['target'] == 1 ].shape[0]) )\n",
    "print( ' Class 0 Size : ' + str(train[ train['target'] == 0 ].shape[0]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>new_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword                                               text  \\\n",
       "0   1     NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [deeds, reason, earthquake, may, allah, forgiv...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [residents, asked, shelter, place, notified, o...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [got, sent, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                            new_text  target  \n",
       "0       deeds reason earthquake may allah forgive us       1  \n",
       "1              forest fire near la ronge sask canada       1  \n",
       "2  residents asked shelter place notified officer...       1  \n",
       "3  13000 people receive wildfires evacuation orde...       1  \n",
       "4  got sent photo ruby alaska smoke wildfires pou...       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "### Function to process data\n",
    "###\n",
    "def textProcessing(dt, textCol, cols = ['id','keyword','text','tokenized','new_text','target'] ):\n",
    "    \n",
    "    # make copy\n",
    "    dt_copy = dt.copy()\n",
    "    \n",
    "    # Get stopwords\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "    # Remove cases\n",
    "    dt_copy['tmp_text'] = dt[textCol].str.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    dt_copy['tmp_text'] = dt_copy.apply(lambda row: row['tmp_text'].translate(str.maketrans('', '', string.punctuation)), axis=1)\n",
    "\n",
    "    # Tokenize\n",
    "    dt_copy['tmp_text'] = dt_copy.apply(lambda row: word_tokenize(row['tmp_text']), axis=1)\n",
    "\n",
    "    # Remove stopwords\n",
    "    dt_copy['tokenized'] = dt_copy['tmp_text'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    \n",
    "    # Merge words\n",
    "    dt_copy['new_text'] = dt_copy['tokenized'].apply( lambda row: ' '.join(row) )\n",
    "    \n",
    "    # Return dataframe\n",
    "    return dt_copy[ cols ]\n",
    "\n",
    "newtrain = textProcessing( train, 'text' )\n",
    "newtrain.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read GloVe vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this chunk takes a bit to run\n",
    "glovePath = '/Users/joaquimlyrio/Downloads/glove/glove.twitter.27B.25d.txt'\n",
    "embeddings_dict = {}\n",
    "with open( glovePath, 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11160975694656372"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "spatial.distance.cosine(np.reshape( embeddings_dict['fire'], (-1,1) ) , \n",
    "                        np.reshape( embeddings_dict['house'], (-1,1) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings for each tweet based on GloVe word embeddings (word -> sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Tweet embedding is the average of the word embeddings\n",
    "def computeGloVeEnsemble( s, embeddings ):\n",
    "    \n",
    "    iCnt = 0\n",
    "    cum = np.zeros( embeddings.get('cool').shape )\n",
    "    \n",
    "    # iterate over words in sentence s\n",
    "    # first word\n",
    "    if s[0] in embeddings:\n",
    "        cum = embeddings.get(s[0])\n",
    "        iCnt = iCnt + 1\n",
    "    \n",
    "    # other words\n",
    "    for w in s[1:]:\n",
    "        if w in embeddings:\n",
    "            cum = cum + embeddings.get(w)\n",
    "            iCnt = iCnt + 1\n",
    "        \n",
    "    return cum / iCnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/py37_ml_lab/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.034642</td>\n",
       "      <td>0.113201</td>\n",
       "      <td>-0.430404</td>\n",
       "      <td>0.489391</td>\n",
       "      <td>-0.853268</td>\n",
       "      <td>-0.306259</td>\n",
       "      <td>1.077509</td>\n",
       "      <td>-0.201949</td>\n",
       "      <td>-0.086634</td>\n",
       "      <td>0.437167</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115637</td>\n",
       "      <td>0.230185</td>\n",
       "      <td>0.121293</td>\n",
       "      <td>0.576177</td>\n",
       "      <td>-0.770221</td>\n",
       "      <td>0.263890</td>\n",
       "      <td>0.306265</td>\n",
       "      <td>-0.436617</td>\n",
       "      <td>0.242523</td>\n",
       "      <td>-0.605413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.567261</td>\n",
       "      <td>-0.202308</td>\n",
       "      <td>-0.182597</td>\n",
       "      <td>-0.002299</td>\n",
       "      <td>-0.911867</td>\n",
       "      <td>0.029404</td>\n",
       "      <td>0.044439</td>\n",
       "      <td>-0.149501</td>\n",
       "      <td>0.482768</td>\n",
       "      <td>-0.156152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251632</td>\n",
       "      <td>0.237843</td>\n",
       "      <td>0.109339</td>\n",
       "      <td>0.164636</td>\n",
       "      <td>0.505321</td>\n",
       "      <td>0.013704</td>\n",
       "      <td>-0.441899</td>\n",
       "      <td>0.354123</td>\n",
       "      <td>-0.906471</td>\n",
       "      <td>0.207224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.600594</td>\n",
       "      <td>0.683745</td>\n",
       "      <td>-0.279191</td>\n",
       "      <td>-0.182460</td>\n",
       "      <td>-0.222019</td>\n",
       "      <td>-0.768375</td>\n",
       "      <td>0.441783</td>\n",
       "      <td>-1.233742</td>\n",
       "      <td>0.358456</td>\n",
       "      <td>0.232217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012621</td>\n",
       "      <td>0.572267</td>\n",
       "      <td>-0.258214</td>\n",
       "      <td>0.147398</td>\n",
       "      <td>-0.713238</td>\n",
       "      <td>0.231145</td>\n",
       "      <td>-0.565903</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>-0.839046</td>\n",
       "      <td>-0.239976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.302245</td>\n",
       "      <td>0.713562</td>\n",
       "      <td>-0.572892</td>\n",
       "      <td>0.051555</td>\n",
       "      <td>-0.331429</td>\n",
       "      <td>-0.620134</td>\n",
       "      <td>0.610448</td>\n",
       "      <td>-1.426075</td>\n",
       "      <td>0.374652</td>\n",
       "      <td>0.257453</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.229748</td>\n",
       "      <td>0.337154</td>\n",
       "      <td>-0.285332</td>\n",
       "      <td>0.569386</td>\n",
       "      <td>-0.700277</td>\n",
       "      <td>-0.067718</td>\n",
       "      <td>-0.516196</td>\n",
       "      <td>0.041319</td>\n",
       "      <td>-0.958743</td>\n",
       "      <td>-0.079488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.440428</td>\n",
       "      <td>0.247316</td>\n",
       "      <td>0.190015</td>\n",
       "      <td>0.146781</td>\n",
       "      <td>-0.571416</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.602678</td>\n",
       "      <td>-0.746124</td>\n",
       "      <td>0.165429</td>\n",
       "      <td>0.126911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120295</td>\n",
       "      <td>0.539177</td>\n",
       "      <td>-0.081443</td>\n",
       "      <td>-0.047724</td>\n",
       "      <td>0.087765</td>\n",
       "      <td>0.124569</td>\n",
       "      <td>-0.118026</td>\n",
       "      <td>0.481347</td>\n",
       "      <td>-0.423808</td>\n",
       "      <td>0.124419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>-0.648327</td>\n",
       "      <td>0.070490</td>\n",
       "      <td>0.380065</td>\n",
       "      <td>-0.042914</td>\n",
       "      <td>-0.387247</td>\n",
       "      <td>0.036434</td>\n",
       "      <td>0.478167</td>\n",
       "      <td>-1.198946</td>\n",
       "      <td>0.579818</td>\n",
       "      <td>0.453287</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100243</td>\n",
       "      <td>0.376008</td>\n",
       "      <td>-0.045428</td>\n",
       "      <td>0.473099</td>\n",
       "      <td>-0.313125</td>\n",
       "      <td>-0.304531</td>\n",
       "      <td>-0.226257</td>\n",
       "      <td>0.297763</td>\n",
       "      <td>-1.010939</td>\n",
       "      <td>-0.234808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>-0.135133</td>\n",
       "      <td>0.096568</td>\n",
       "      <td>-0.276388</td>\n",
       "      <td>-0.126443</td>\n",
       "      <td>-0.288394</td>\n",
       "      <td>0.141316</td>\n",
       "      <td>0.764192</td>\n",
       "      <td>-0.630848</td>\n",
       "      <td>0.299536</td>\n",
       "      <td>-0.072727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019655</td>\n",
       "      <td>0.220042</td>\n",
       "      <td>0.173761</td>\n",
       "      <td>0.262831</td>\n",
       "      <td>-0.165517</td>\n",
       "      <td>0.061640</td>\n",
       "      <td>-0.072648</td>\n",
       "      <td>0.181287</td>\n",
       "      <td>-0.851537</td>\n",
       "      <td>-0.043328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>-1.168000</td>\n",
       "      <td>-0.450815</td>\n",
       "      <td>0.532550</td>\n",
       "      <td>-0.399675</td>\n",
       "      <td>-1.363600</td>\n",
       "      <td>-0.095002</td>\n",
       "      <td>0.954495</td>\n",
       "      <td>-0.814990</td>\n",
       "      <td>0.556580</td>\n",
       "      <td>0.270772</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002895</td>\n",
       "      <td>0.338860</td>\n",
       "      <td>0.395045</td>\n",
       "      <td>1.074420</td>\n",
       "      <td>0.028868</td>\n",
       "      <td>-0.702900</td>\n",
       "      <td>-0.445012</td>\n",
       "      <td>0.069295</td>\n",
       "      <td>-1.341100</td>\n",
       "      <td>0.638075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>-0.287354</td>\n",
       "      <td>0.493432</td>\n",
       "      <td>-0.094455</td>\n",
       "      <td>-0.263650</td>\n",
       "      <td>0.140191</td>\n",
       "      <td>-0.228304</td>\n",
       "      <td>0.218582</td>\n",
       "      <td>-0.772906</td>\n",
       "      <td>0.312802</td>\n",
       "      <td>-0.002916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.336800</td>\n",
       "      <td>0.562909</td>\n",
       "      <td>0.074350</td>\n",
       "      <td>0.353500</td>\n",
       "      <td>-0.345130</td>\n",
       "      <td>0.084602</td>\n",
       "      <td>0.363871</td>\n",
       "      <td>0.330678</td>\n",
       "      <td>-0.892794</td>\n",
       "      <td>-0.284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>-0.210782</td>\n",
       "      <td>0.028216</td>\n",
       "      <td>-0.000312</td>\n",
       "      <td>-0.517072</td>\n",
       "      <td>-0.337078</td>\n",
       "      <td>-0.333011</td>\n",
       "      <td>0.361668</td>\n",
       "      <td>-0.830756</td>\n",
       "      <td>0.631351</td>\n",
       "      <td>0.048237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.513970</td>\n",
       "      <td>0.231099</td>\n",
       "      <td>0.292712</td>\n",
       "      <td>0.611585</td>\n",
       "      <td>-0.013370</td>\n",
       "      <td>-0.244303</td>\n",
       "      <td>-0.745605</td>\n",
       "      <td>-0.096155</td>\n",
       "      <td>-1.403071</td>\n",
       "      <td>0.199998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -0.034642  0.113201 -0.430404  0.489391 -0.853268 -0.306259  1.077509   \n",
       "1    -0.567261 -0.202308 -0.182597 -0.002299 -0.911867  0.029404  0.044439   \n",
       "2    -0.600594  0.683745 -0.279191 -0.182460 -0.222019 -0.768375  0.441783   \n",
       "3    -0.302245  0.713562 -0.572892  0.051555 -0.331429 -0.620134  0.610448   \n",
       "4    -0.440428  0.247316  0.190015  0.146781 -0.571416  0.001253  0.602678   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7608 -0.648327  0.070490  0.380065 -0.042914 -0.387247  0.036434  0.478167   \n",
       "7609 -0.135133  0.096568 -0.276388 -0.126443 -0.288394  0.141316  0.764192   \n",
       "7610 -1.168000 -0.450815  0.532550 -0.399675 -1.363600 -0.095002  0.954495   \n",
       "7611 -0.287354  0.493432 -0.094455 -0.263650  0.140191 -0.228304  0.218582   \n",
       "7612 -0.210782  0.028216 -0.000312 -0.517072 -0.337078 -0.333011  0.361668   \n",
       "\n",
       "            7         8         9   ...        15        16        17  \\\n",
       "0    -0.201949 -0.086634  0.437167  ... -0.115637  0.230185  0.121293   \n",
       "1    -0.149501  0.482768 -0.156152  ...  0.251632  0.237843  0.109339   \n",
       "2    -1.233742  0.358456  0.232217  ...  0.012621  0.572267 -0.258214   \n",
       "3    -1.426075  0.374652  0.257453  ... -0.229748  0.337154 -0.285332   \n",
       "4    -0.746124  0.165429  0.126911  ...  0.120295  0.539177 -0.081443   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7608 -1.198946  0.579818  0.453287  ... -0.100243  0.376008 -0.045428   \n",
       "7609 -0.630848  0.299536 -0.072727  ... -0.019655  0.220042  0.173761   \n",
       "7610 -0.814990  0.556580  0.270772  ... -0.002895  0.338860  0.395045   \n",
       "7611 -0.772906  0.312802 -0.002916  ...  0.336800  0.562909  0.074350   \n",
       "7612 -0.830756  0.631351  0.048237  ... -0.513970  0.231099  0.292712   \n",
       "\n",
       "            18        19        20        21        22        23        24  \n",
       "0     0.576177 -0.770221  0.263890  0.306265 -0.436617  0.242523 -0.605413  \n",
       "1     0.164636  0.505321  0.013704 -0.441899  0.354123 -0.906471  0.207224  \n",
       "2     0.147398 -0.713238  0.231145 -0.565903  0.002023 -0.839046 -0.239976  \n",
       "3     0.569386 -0.700277 -0.067718 -0.516196  0.041319 -0.958743 -0.079488  \n",
       "4    -0.047724  0.087765  0.124569 -0.118026  0.481347 -0.423808  0.124419  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "7608  0.473099 -0.313125 -0.304531 -0.226257  0.297763 -1.010939 -0.234808  \n",
       "7609  0.262831 -0.165517  0.061640 -0.072648  0.181287 -0.851537 -0.043328  \n",
       "7610  1.074420  0.028868 -0.702900 -0.445012  0.069295 -1.341100  0.638075  \n",
       "7611  0.353500 -0.345130  0.084602  0.363871  0.330678 -0.892794 -0.284000  \n",
       "7612  0.611585 -0.013370 -0.244303 -0.745605 -0.096155 -1.403071  0.199998  \n",
       "\n",
       "[7613 rows x 25 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Compute tweet embeddings based on GloVe\n",
    "dt_copy = newtrain[['tokenized']].copy()\n",
    "gloveSeries = dt_copy.apply(lambda row: computeGloVeEnsemble( row['tokenized'], embeddings=embeddings_dict ), axis=1)\n",
    "\n",
    "# convert one column to multiple columns (no. of features -> 25)\n",
    "gloveEmbeds = pd.DataFrame(gloveSeries)\n",
    "gloveEmbeds = gloveEmbeds[0].apply(pd.Series)\n",
    "gloveEmbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7610, 26)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gloveEmbeds['target'] = newtrain['target']\n",
    "gloveEmbeds = gloveEmbeds.dropna()\n",
    "gloveEmbeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train/Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:(5707, 25)\n",
      "Proportion of class 0 in train: 56.82%\n",
      "Proportion of class 1 in train: 43.18%\n",
      "\n",
      "Test shape:(1903, 25)\n",
      "Proportion of class 0 in train: 57.65%\n",
      "Proportion of class 1 in train: 42.35%\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset for cross validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(gloveEmbeds.loc[:, gloveEmbeds.columns != 'target'], \n",
    "                                                    gloveEmbeds.target, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=123)\n",
    "\n",
    "\n",
    "print('Train shape:' + str(X_train.shape) )\n",
    "print('Proportion of class 0 in train: ' + str( np.round( 100*np.sum(y_train==0) / len(y_train), 2 ) ) + '%' )\n",
    "print('Proportion of class 1 in train: ' + str( np.round( 100*np.sum(y_train==1) / len(y_train), 2 ) ) + '%' )\n",
    "print('')\n",
    "print('Test shape:' + str(X_test.shape) )\n",
    "print('Proportion of class 0 in train: ' + str( np.round( 100*np.sum(y_test==0) / len(y_test), 2 ) ) + '%' )\n",
    "print('Proportion of class 1 in train: ' + str( np.round( 100*np.sum(y_test==1) / len(y_test), 2 ) ) + '%' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bernoulli Naive Bayes\n",
      "\n",
      "Train 0/1 Accuracy: 0.7333099702120204\n",
      "Test 0/1 Accuracy: 0.7299001576458224\n",
      "\n",
      "Train F1 Score: 0.7026182102383745\n",
      "Test F1 Score: 0.6933174224343676\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Fit Bernoulli Naive-Bayes\n",
    "print('')\n",
    "print('Bernoulli Naive Bayes')\n",
    "print('')\n",
    "\n",
    "clf_nb = BernoulliNB().fit( X_train, y_train )\n",
    "clf_nb\n",
    "\n",
    "##\n",
    "## Predict on train / test set\n",
    "print( 'Train 0/1 Accuracy: ' + str( np.mean( clf_nb.predict(X_train) == y_train ) ) )\n",
    "print( 'Test 0/1 Accuracy: ' + str( np.mean( clf_nb.predict(X_test) == y_test ) ) )\n",
    "print('')\n",
    "print( 'Train F1 Score: ' + str( metrics.f1_score( y_true = np.reshape(y_train.to_numpy(), (-1, 1)),\n",
    "                                                   y_pred = clf_nb.predict(X_train) ) ) )\n",
    "print( 'Test F1 Score: ' + str( metrics.f1_score( y_true = np.reshape(y_test.to_numpy(), (-1, 1)),\n",
    "                                                  y_pred = clf_nb.predict(X_test) ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge Classifier\n",
      "\n",
      "Train 0/1 Accuracy: 0.779568950411775\n",
      "Test 0/1 Accuracy: 0.7682606410930111\n",
      "\n",
      "Train F1 Score: 0.72472647702407\n",
      "Test F1 Score: 0.7100591715976331\n"
     ]
    }
   ],
   "source": [
    "print('')\n",
    "print('Ridge Classifier')\n",
    "print('')\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "clf_ridge = RidgeClassifierCV( alphas=[1e-3, 1e-2, 1e-1, 1, 5, 10], cv = 5 ).fit(X_train, y_train)\n",
    "\n",
    "print( 'Train 0/1 Accuracy: ' + str( np.mean( clf_ridge.predict(X_train) == y_train ) ) )\n",
    "print( 'Test 0/1 Accuracy: ' + str( np.mean( clf_ridge.predict(X_test) == y_test ) ) )\n",
    "print('')\n",
    "print( 'Train F1 Score: ' + str( metrics.f1_score( y_true = np.reshape(y_train.to_numpy(), (-1, 1)),\n",
    "                                                   y_pred = clf_ridge.predict(X_train) ) ) )\n",
    "print( 'Test F1 Score: ' + str( metrics.f1_score( y_true = np.reshape(y_test.to_numpy(), (-1, 1)),\n",
    "                                                  y_pred = clf_ridge.predict(X_test) ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Feed-Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                416       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,289\n",
      "Trainable params: 2,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "##\n",
    "## Define model's architecture\n",
    "model = tf.keras.Sequential()\n",
    "model.add( tf.keras.layers.Dense(16, activation='tanh', input_shape=(X_train.shape[1],) ) )\n",
    "model.add( tf.keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],) ) )\n",
    "model.add( tf.keras.layers.Dense(32, activation='tanh', input_shape=(X_train.shape[1],) ) )\n",
    "model.add( tf.keras.layers.Dense(8, activation='relu', input_shape=(X_train.shape[1],) ) )\n",
    "model.add( tf.keras.layers.Dense(1, activation='sigmoid' ) )\n",
    "\n",
    "##\n",
    "## Compile model\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "#               optimizer=keras.optimizers.RMSprop(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "##\n",
    "## Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5707 samples, validate on 1903 samples\n",
      "Epoch 1/20\n",
      "5707/5707 [==============================] - 1s 196us/sample - loss: 0.5565 - accuracy: 0.7102 - val_loss: 0.4899 - val_accuracy: 0.7714\n",
      "Epoch 2/20\n",
      "5707/5707 [==============================] - 0s 47us/sample - loss: 0.4776 - accuracy: 0.7813 - val_loss: 0.4726 - val_accuracy: 0.7751\n",
      "Epoch 3/20\n",
      "5707/5707 [==============================] - 0s 47us/sample - loss: 0.4643 - accuracy: 0.7899 - val_loss: 0.4651 - val_accuracy: 0.7851\n",
      "Epoch 4/20\n",
      "5707/5707 [==============================] - 0s 43us/sample - loss: 0.4577 - accuracy: 0.7927 - val_loss: 0.4621 - val_accuracy: 0.7814\n",
      "Epoch 5/20\n",
      "5707/5707 [==============================] - 0s 46us/sample - loss: 0.4539 - accuracy: 0.7932 - val_loss: 0.4617 - val_accuracy: 0.7898\n",
      "Epoch 6/20\n",
      "5707/5707 [==============================] - 0s 44us/sample - loss: 0.4498 - accuracy: 0.7953 - val_loss: 0.4567 - val_accuracy: 0.7924\n",
      "Epoch 7/20\n",
      "5707/5707 [==============================] - 0s 46us/sample - loss: 0.4440 - accuracy: 0.7994 - val_loss: 0.4564 - val_accuracy: 0.7956\n",
      "Epoch 8/20\n",
      "5707/5707 [==============================] - 0s 44us/sample - loss: 0.4419 - accuracy: 0.8018 - val_loss: 0.4556 - val_accuracy: 0.7935\n",
      "Epoch 9/20\n",
      "5707/5707 [==============================] - 0s 43us/sample - loss: 0.4392 - accuracy: 0.8025 - val_loss: 0.4685 - val_accuracy: 0.7793\n",
      "Epoch 10/20\n",
      "5707/5707 [==============================] - 0s 43us/sample - loss: 0.4351 - accuracy: 0.8045 - val_loss: 0.4522 - val_accuracy: 0.7977\n",
      "Epoch 11/20\n",
      "5707/5707 [==============================] - 0s 43us/sample - loss: 0.4331 - accuracy: 0.8073 - val_loss: 0.4585 - val_accuracy: 0.7930\n",
      "Epoch 12/20\n",
      "5707/5707 [==============================] - 0s 51us/sample - loss: 0.4322 - accuracy: 0.8059 - val_loss: 0.4722 - val_accuracy: 0.7814\n",
      "Epoch 13/20\n",
      "5707/5707 [==============================] - 0s 54us/sample - loss: 0.4300 - accuracy: 0.8088 - val_loss: 0.4599 - val_accuracy: 0.7940\n",
      "Epoch 14/20\n",
      "5707/5707 [==============================] - 0s 54us/sample - loss: 0.4285 - accuracy: 0.8090 - val_loss: 0.4578 - val_accuracy: 0.7919\n",
      "Epoch 15/20\n",
      "5707/5707 [==============================] - 0s 44us/sample - loss: 0.4276 - accuracy: 0.8108 - val_loss: 0.4624 - val_accuracy: 0.7914\n",
      "Epoch 16/20\n",
      "5707/5707 [==============================] - 0s 52us/sample - loss: 0.4257 - accuracy: 0.8132 - val_loss: 0.4551 - val_accuracy: 0.7966\n",
      "Epoch 17/20\n",
      "5707/5707 [==============================] - 0s 54us/sample - loss: 0.4219 - accuracy: 0.8143 - val_loss: 0.4604 - val_accuracy: 0.7993\n",
      "Epoch 18/20\n",
      "5707/5707 [==============================] - 0s 53us/sample - loss: 0.4222 - accuracy: 0.8151 - val_loss: 0.4619 - val_accuracy: 0.7956\n",
      "Epoch 19/20\n",
      "5707/5707 [==============================] - 0s 54us/sample - loss: 0.4186 - accuracy: 0.8143 - val_loss: 0.4601 - val_accuracy: 0.7930\n",
      "Epoch 20/20\n",
      "5707/5707 [==============================] - 0s 53us/sample - loss: 0.4172 - accuracy: 0.8162 - val_loss: 0.4562 - val_accuracy: 0.7977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a409fca50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train Model\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          epochs=20,\n",
    "          validation_data=(X_test, y_test),\n",
    "          verbose=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neural Net - Feed-forward\n",
      "\n",
      "Train 0/1 Accuracy: 0.8239004731032066\n",
      "Test 0/1 Accuracy: 0.7976878612716763\n",
      "\n",
      "Train F1 Score: 0.7802317953203586\n",
      "Test F1 Score: 0.7468770545693624\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Predict on train / test set\n",
    "print('')\n",
    "print('Neural Net - Feed-forward')\n",
    "print('')\n",
    "\n",
    "# get predictions for in/out of sample\n",
    "preds_train = np.reshape( np.round(model.predict(X_train)), (-1,) )\n",
    "preds_test  = np.reshape( np.round(model.predict(X_test)), (-1,) )\n",
    "\n",
    "print( 'Train 0/1 Accuracy: ' + str( np.mean( preds_train == y_train ) ) )\n",
    "print( 'Test 0/1 Accuracy: ' + str( np.mean( preds_test == y_test ) ) )\n",
    "print('')\n",
    "print( 'Train F1 Score: ' + str( metrics.f1_score( y_true = y_train,\n",
    "                                                   y_pred = preds_train ) ) )\n",
    "print( 'Test F1 Score: ' + str( metrics.f1_score( y_true = y_test,\n",
    "                                                   y_pred = preds_test ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
